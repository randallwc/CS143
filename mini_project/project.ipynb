{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "998c56a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da8aff05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CS 143 - SPRING 2021 -- MINIPROJECT\n",
    "# Skeleton Code\n",
    "\n",
    "# AT THE BEGINNING OF EACH SESSION, COPY AND PASTE ALL OF THE FOLLOWING CODE\n",
    "# UNTIL YOU REACH \"STOP COPY/PASTE\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import string\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from pyspark.sql.functions import broadcast, col, date_format, desc, explode, from_unixtime, log, to_date, udf\n",
    "\n",
    "#         master(\"spark://spark-master:7077\").\\\n",
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"solution\").\\\n",
    "        master(\"local\").\\\n",
    "        config(\"spark.executor.memory\", \"2048m\").\\\n",
    "        getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "_PUNCTUATION = \"\".join(string.punctuation + \"â€™\")\n",
    "\n",
    "# You don't need to do anything with the stopwords, so don't be intimidated.\n",
    "stopwords = frozenset([\"'ll\", \"'ve\", \"I\", \"a\", \"a's\", \"able\", \"about\", \"above\",\n",
    "    \"abst\", \"accordance\", \"according\", \"accordingly\", \"across\", \"act\",\n",
    "    \"actually\", \"added\", \"adj\", \"affected\", \"affecting\", \"affects\", \"after\",\n",
    "    \"afterwards\", \"again\", \"against\", \"ah\", \"ain't\", \"all\", \"allow\", \"allows\", \n",
    "    \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\", \"am\",\n",
    "    \"among\", \"amongst\", \"an\", \"and\", \"announce\", \"another\", \"any\", \"anybody\",\n",
    "    \"anyhow\", \"anymore\", \"anyone\", \"anything\", \"anyway\", \"anyways\", \"anywhere\",\n",
    "    \"apart\", \"apparently\", \"appear\", \"appreciate\", \"appropriate\",\n",
    "    \"approximately\", \"are\", \"aren\", \"aren't\", \"arent\", \"arise\", \"around\", \"as\",\n",
    "    \"aside\", \"ask\", \"asking\", \"associated\", \"at\", \"auth\", \"available\", \"away\",\n",
    "    \"awfully\", \"b\", \"back\", \"be\", \"became\", \"because\", \"become\", \"becomes\",\n",
    "    \"becoming\", \"been\", \"before\", \"beforehand\", \"begin\", \"beginning\",\n",
    "    \"beginnings\", \"begins\", \"behind\", \"being\", \"believe\", \"below\", \"beside\",\n",
    "    \"besides\", \"best\", \"better\", \"between\", \"beyond\", \"biol\", \"both\", \"brief\",\n",
    "    \"briefly\", \"but\", \"by\", \"c\", \"c'mon\", \"c's\", \"ca\", \"came\", \"can\", \"can't\",\n",
    "    \"cannot\", \"cant\", \"cause\", \"causes\", \"certain\", \"certainly\", \"changes\",\n",
    "    \"clearly\", \"co\", \"com\", \"come\", \"comes\", \"concerning\", \"consequently\",\n",
    "    \"consider\", \"considering\", \"contain\", \"containing\", \"contains\",\n",
    "    \"corresponding\", \"could\", \"couldn't\", \"couldnt\", \"course\", \"currently\", \"d\",\n",
    "    \"date\", \"definitely\", \"described\", \"despite\", \"did\", \"didn't\", \"different\",\n",
    "    \"do\", \"does\", \"doesn't\", \"doing\", \"don't\", \"done\", \"down\", \"downwards\",\n",
    "    \"due\", \"during\", \"e\", \"each\", \"ed\", \"edu\", \"effect\", \"eg\", \"eight\", \"eighty\",\n",
    "    \"either\", \"else\", \"elsewhere\", \"end\", \"ending\", \"enough\", \"entirely\",\n",
    "    \"especially\", \"et\", \"et-al\", \"etc\", \"even\", \"ever\", \"every\", \"everybody\",\n",
    "    \"everyone\", \"everything\", \"everywhere\", \"ex\", \"exactly\", \"example\",\n",
    "    \"except\", \"f\", \"far\", \"few\", \"ff\", \"fifth\", \"first\", \"five\", \"fix\",\n",
    "    \"followed\", \"following\", \"follows\", \"for\", \"former\", \"formerly\", \"forth\",\n",
    "    \"found\", \"four\", \"from\", \"further\", \"furthermore\", \"g\", \"gave\", \"get\",\n",
    "    \"gets\", \"getting\", \"give\", \"given\", \"gives\", \"giving\", \"go\", \"goes\",\n",
    "    \"going\", \"gone\", \"got\", \"gotten\", \"greetings\", \"h\", \"had\", \"hadn't\",\n",
    "    \"happens\", \"hardly\", \"has\", \"hasn't\", \"have\", \"haven't\", \"having\", \"he\",\n",
    "    \"he'd\", \"he'll\", \"he's\", \"hed\", \"hello\", \"help\", \"hence\", \"her\", \"here\",\n",
    "    \"here's\", \"hereafter\", \"hereby\", \"herein\", \"heres\", \"hereupon\", \"hers\",\n",
    "    \"herself\", \"hes\", \"hi\", \"hid\", \"him\", \"himself\", \"his\", \"hither\", \"home\",\n",
    "    \"hopefully\", \"how\", \"how's\", \"howbeit\", \"however\", \"hundred\", \"i\", \"i'd\",\n",
    "    \"i'll\", \"i'm\", \"i've\", \"id\", \"ie\", \"if\", \"ignored\", \"im\", \"immediate\", \n",
    "    \"immediately\", \"importance\", \"important\", \"in\", \"inasmuch\", \"inc\", \"indeed\",\n",
    "    \"index\", \"indicate\", \"indicated\", \"indicates\", \"information\", \"inner\", \n",
    "    \"insofar\", \"instead\", \"into\", \"invention\", \"inward\", \"is\", \"isn't\", \"it\",\n",
    "    \"it'd\", \"it'll\", \"it's\", \"itd\", \"its\", \"itself\", \"j\", \"just\", \"k\", \"keep\",\n",
    "    \"keeps\", \"kept\", \"kg\", \"km\", \"know\", \"known\", \"knows\", \"l\", \"largely\",\n",
    "    \"last\", \"lately\", \"later\", \"latter\", \"latterly\", \"least\", \"less\", \"lest\",\n",
    "    \"let\", \"let's\", \"lets\", \"like\", \"liked\", \"likely\", \"line\", \"little\", \"look\",\n",
    "    \"looking\", \"looks\", \"ltd\", \"m\", \"made\", \"mainly\", \"make\", \"makes\", \"many\",\n",
    "    \"may\", \"maybe\", \"me\", \"mean\", \"means\", \"meantime\", \"meanwhile\", \"merely\",\n",
    "    \"mg\", \"might\", \"million\", \"miss\", \"ml\", \"more\", \"moreover\", \"most\", \"mostly\",\n",
    "    \"mr\", \"mrs\", \"much\", \"mug\", \"must\", \"mustn't\", \"my\", \"myself\", \"n\", \"na\",\n",
    "    \"name\", \"namely\", \"nay\", \"nd\", \"near\", \"nearly\", \"necessarily\", \"necessary\",\n",
    "    \"need\", \"needs\", \"neither\", \"never\", \"nevertheless\", \"new\", \"next\", \"nine\",\n",
    "    \"ninety\", \"no\", \"nobody\", \"non\", \"none\", \"nonetheless\", \"noone\", \"nor\",\n",
    "    \"normally\", \"nos\", \"not\", \"noted\", \"nothing\", \"novel\", \"now\", \"nowhere\",\n",
    "    \"o\", \"obtain\", \"obtained\", \"obviously\", \"of\", \"off\", \"often\", \"oh\", \"ok\",\n",
    "    \"okay\", \"old\", \"omitted\", \"on\", \"once\", \"one\", \"ones\", \"only\", \"onto\", \"or\",\n",
    "    \"ord\", \"other\", \"others\", \"otherwise\", \"ought\", \"our\", \"ours\", \"ourselves\",\n",
    "    \"out\", \"outside\", \"over\", \"overall\", \"owing\", \"own\", \"p\", \"page\", \"pages\",\n",
    "    \"part\", \"particular\", \"particularly\", \"past\", \"per\", \"perhaps\", \"placed\",\n",
    "    \"please\", \"plus\", \"poorly\", \"possible\", \"possibly\", \"potentially\", \"pp\",\n",
    "    \"predominantly\", \"present\", \"presumably\", \"previously\", \"primarily\",\n",
    "    \"probably\", \"promptly\", \"proud\", \"provides\", \"put\", \"q\", \"que\", \"quickly\",\n",
    "    \"quite\", \"qv\", \"r\", \"ran\", \"rather\", \"rd\", \"re\", \"readily\", \"really\", \n",
    "    \"reasonably\", \"recent\", \"recently\", \"ref\", \"refs\", \"regarding\",\n",
    "    \"regardless\", \"regards\", \"related\", \"relatively\", \"research\",\n",
    "    \"respectively\", \"resulted\", \"resulting\", \"results\", \"right\", \"run\", \"s\",\n",
    "    \"said\", \"same\", \"saw\", \"say\", \"saying\", \"says\", \"sec\", \"second\",\n",
    "    \"secondly\", \"section\", \"see\", \"seeing\", \"seem\", \"seemed\", \"seeming\",\n",
    "    \"seems\", \"seen\", \"self\", \"selves\", \"sensible\", \"sent\", \"serious\",\n",
    "    \"seriously\", \"seven\", \"several\", \"shall\", \"shan't\", \"she\", \"she'd\",\n",
    "    \"she'll\", \"she's\", \"shed\", \"shes\", \"should\", \"shouldn't\", \"show\", \"showed\",\n",
    "    \"shown\", \"showns\", \"shows\", \"significant\", \"significantly\", \"similar\",\n",
    "    \"similarly\", \"since\", \"six\", \"slightly\", \"so\", \"some\", \"somebody\",\n",
    "    \"somehow\", \"someone\", \"somethan\", \"something\", \"sometime\", \"sometimes\",\n",
    "    \"somewhat\", \"somewhere\", \"soon\", \"sorry\", \"specifically\", \"specified\",\n",
    "    \"specify\", \"specifying\", \"still\", \"stop\", \"strongly\", \"sub\",\n",
    "    \"substantially\", \"successfully\", \"such\", \"sufficiently\", \"suggest\", \"sup\",\n",
    "    \"sure\", \"t\", \"t's\", \"take\", \"taken\", \"taking\", \"tell\", \"tends\", \"th\",\n",
    "    \"than\", \"thank\", \"thanks\", \"thanx\", \"that\", \"that'll\", \"that's\", \"that've\",\n",
    "    \"thats\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"thence\",\n",
    "    \"there\", \"there'll\", \"there's\", \"there've\", \"thereafter\", \"thereby\",\n",
    "    \"thered\", \"therefore\", \"therein\", \"thereof\", \"therere\", \"theres\",\n",
    "    \"thereto\", \"thereupon\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\",\n",
    "    \"they've\", \"theyd\", \"theyre\", \"think\", \"third\", \"this\", \"thorough\",\n",
    "    \"thoroughly\", \"those\", \"thou\", \"though\", \"thoughh\", \"thousand\", \"three\",\n",
    "    \"throug\", \"through\", \"throughout\", \"thru\", \"thus\", \"til\", \"tip\", \"to\",\n",
    "    \"together\", \"too\", \"took\", \"toward\", \"towards\", \"tried\", \"tries\", \"truly\",\n",
    "    \"try\", \"trying\", \"ts\", \"twice\", \"two\", \"u\", \"un\", \"under\", \"unfortunately\",\n",
    "    \"unless\", \"unlike\", \"unlikely\", \"until\", \"unto\", \"up\", \"upon\", \"ups\", \"us\",\n",
    "    \"use\", \"used\", \"useful\", \"usefully\", \"usefulness\", \"uses\", \"using\",\n",
    "    \"usually\", \"v\", \"value\", \"various\", \"very\", \"via\", \"viz\", \"vol\", \"vols\",\n",
    "    \"vs\", \"w\", \"want\", \"wants\", \"was\", \"wasn't\", \"wasnt\", \"way\", \"we\", \"we'd\",\n",
    "    \"we'll\", \"we're\", \"we've\", \"wed\", \"welcome\", \"well\", \"went\", \"were\",\n",
    "    \"weren't\", \"werent\", \"what\", \"what'll\", \"what's\", \"whatever\", \"whats\",\n",
    "    \"when\", \"when's\", \"whence\", \"whenever\", \"where\", \"where's\", \"whereafter\",\n",
    "    \"whereas\", \"whereby\", \"wherein\", \"wheres\", \"whereupon\", \"wherever\",\n",
    "    \"whether\", \"which\", \"while\", \"whim\", \"whither\", \"who\", \"who'll\", \"who's\",\n",
    "    \"whod\", \"whoever\", \"whole\", \"whom\", \"whomever\", \"whos\", \"whose\", \"why\",\n",
    "    \"why's\", \"widely\", \"will\", \"willing\", \"wish\", \"with\", \"within\", \"without\",\n",
    "    \"won't\", \"wonder\", \"wont\", \"words\", \"world\", \"would\", \"wouldn't\",\n",
    "    \"wouldnt\", \"www\", \"x\", \"y\", \"yes\", \"yet\", \"you\", \"you'd\", \"you'll\",\n",
    "    \"you're\", \"you've\", \"youd\", \"your\", \"youre\", \"yours\", \"yourself\",\n",
    "    \"yourselves\", \"z\", \"zero\", \"ucla\", \"class\", \"people\", \"classes\", \"lol\",\n",
    "    \"thanks\", \"dm\"])\n",
    "\n",
    "@udf(ArrayType(StringType()))\n",
    "def cleantext(x):\n",
    "    # Convert to lower case.\n",
    "    text = x.lower()\n",
    "    # Split on space\n",
    "    tokens = text.split(' ')\n",
    "    # Remove stopwords\n",
    "    nostops = ' '.join([token for token in tokens if token not in _STOPWORDS.value])\n",
    "    # Remove punctuation.\n",
    "    tokens = ''.join([c for c in nostops if c not in _PUNCTUATION]).split(' ')\n",
    "    # Remove empty tokens.\n",
    "    tokens = [token for token in tokens if token]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# UDF is automatically sent to all workers, but stopwords frozenset is not.\n",
    "# We have to distribute it to the workers ourself:\n",
    "_STOPWORDS = sc.broadcast(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2f47b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "# Tips for Writing and Debugging #\n",
    "##################################\n",
    "# 1. If you want to check that you are on the right track, use the\n",
    "#    head(10) method. For example:\n",
    "#    ucla.select(\"subreddit\").show()\n",
    "#    or ucla.select(\"subreddit\").head(10)\n",
    "#    will output the subreddit field to the screen.\n",
    "#    Remember what we discussed about LAZY EVALUATION.\n",
    "# 2. You can print the schema at any time by adding .printSchema()\n",
    "# 3. Words in asterisks ** give a clue as to what operator to use. (use documentation)\n",
    "#    You can probably use other operators as well if you find a better one.\n",
    "# 4. The imports come from my solution, and this should also give a clue. This does NOT\n",
    "#    mean you can only use my operators. You may input your own.\n",
    "# 5. I've prefixed each step with a variable containing your output. You should only need\n",
    "#    to write one line for each. You are free to use more, but note that you may also be doing\n",
    "#    something wrong, it just depends on your method.\n",
    "# 6. Be careful with aliases.\n",
    "# 8. The usual aggregation functions are implemented: count, countDistinc, min, max, avg etc. These\n",
    "#    are imported as \"F\".\n",
    "# 9. TF-IDF is pretty naive for this problem. Don't expect to be impressed with the results.\n",
    "##################################\n",
    "\n",
    "###########################################\n",
    "# YOU WILL WRITE CODE FOR STEPS 3, 5-14\n",
    "###########################################\n",
    "\n",
    "# STEP 1/2: Load the data and subset to only the three columns that we will\n",
    "# use in this prokect. \"body\", the text of the comment, \"created_utc\" which\n",
    "# is the  Unix timestamp when it was submitted, and \"id\" which is a unique\n",
    "# identifier for each comment. We will also drop duplicates because the\n",
    "# researcher fetched some comments multipe times.\n",
    "ucla = spark.read.json(\"data/*.json\")  # creates a DataFrame\n",
    "ucla = ucla.select([\"id\", \"created_utc\", \"body\"]).dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87ac44a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+--------------------+\n",
      "|     id|created_utc|                body|\n",
      "+-------+-----------+--------------------+\n",
      "|ezsdavt| 1568087811|If anyone is look...|\n",
      "|ezxy2ms| 1568252600|            Jack off|\n",
      "|f00o5zr| 1568304235|    Whereâ€™s the joke|\n",
      "|f026ytg| 1568324562|thatâ€™s the minor ...|\n",
      "|f02oiym| 1568334930|Oh yea that perso...|\n",
      "+-------+-----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ucla.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94219162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+--------------------+\n",
      "|     id|created_utc|                body|\n",
      "+-------+-----------+--------------------+\n",
      "|ezsdavt| 1568087811|If anyone is look...|\n",
      "|ezxy2ms| 1568252600|            Jack off|\n",
      "|f00o5zr| 1568304235|    Whereâ€™s the joke|\n",
      "|f026ytg| 1568324562|thatâ€™s the minor ...|\n",
      "|f02oiym| 1568334930|Oh yea that perso...|\n",
      "+-------+-----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# **TODO** STEP 3A (originally Step 6): Remove comments whose entire text is \n",
    "# \"[deleted]\", \"deleted\", \"[removed]\" and \"removed\" which just \n",
    "# means that the comment was deleted or removed and is not interesting to us.\n",
    "# What operator would we use to find these?\n",
    "result3a = ucla.where(\"body not in ('[deleted]', 'deleted', '[removed]', 'removed')\")\n",
    "result3a.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac6540ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+--------------------+--------------------+\n",
      "|     id|created_utc|                body|        cleaned_text|\n",
      "+-------+-----------+--------------------+--------------------+\n",
      "|ezsdavt| 1568087811|If anyone is look...|     [tickets, ðŸ‘€ðŸ‘€]|\n",
      "|ezxy2ms| 1568252600|            Jack off|              [jack]|\n",
      "|f00o5zr| 1568304235|    Whereâ€™s the joke|      [wheres, joke]|\n",
      "|f026ytg| 1568324562|thatâ€™s the minor ...|[thats, minor, th...|\n",
      "|f02oiym| 1568334930|Oh yea that perso...|[yea, person, lgb...|\n",
      "+-------+-----------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# **TODO** STEP 3B: Use the UDF to clean the text of each comment by converting\n",
    "# to lower case and splitting on a space after removing stopwords.  Note that\n",
    "# splitting on a space gives us a LIST of tokens into the column. Note that\n",
    "# cleantext is NOT perfect, you will see some bugs.\n",
    "# HINT: Research withColumn operator, which creates a new column.\n",
    "# NOTE: You may need to filter out empty rows first.\n",
    "\n",
    "result3b = result3a.withColumn('cleaned_text',cleantext('body'))\n",
    "result3b.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad7ef339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+-------+\n",
      "|     id|                body|        cleaned_text|  month|\n",
      "+-------+--------------------+--------------------+-------+\n",
      "|ezsdavt|If anyone is look...|     [tickets, ðŸ‘€ðŸ‘€]|2019-09|\n",
      "|ezxy2ms|            Jack off|              [jack]|2019-09|\n",
      "|f00o5zr|    Whereâ€™s the joke|      [wheres, joke]|2019-09|\n",
      "|f026ytg|thatâ€™s the minor ...|[thats, minor, th...|2019-09|\n",
      "|f02oiym|Oh yea that perso...|[yea, person, lgb...|2019-09|\n",
      "+-------+--------------------+--------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# STEP 4: There is a Unix timestamp (a long) in the field \"created_utc\". Let's\n",
    "# create a new column \"month\" that converts the timestamp to a Python datetime object,\n",
    "# and also only retains the MONTH and YEAR as \"2019-11\".\n",
    "# Asking you to write code to convert dates/times would be cruel and unusual punishment,\n",
    "# so I have done it for you.\n",
    "result4 = result3b.withColumn(\"month\", date_format(to_date(from_unixtime(\"created_utc\"),\n",
    "            \"yyyy-MM-dd HH:mm:SS\"), \"yyyy-MM\")).drop(result3b.created_utc)\n",
    "result4.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c54f303a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-------+-------+\n",
      "|     id|                body|  month|   word|\n",
      "+-------+--------------------+-------+-------+\n",
      "|ezsdavt|If anyone is look...|2019-09|tickets|\n",
      "|ezsdavt|If anyone is look...|2019-09|   ðŸ‘€ðŸ‘€|\n",
      "|ezxy2ms|            Jack off|2019-09|   jack|\n",
      "|f00o5zr|    Whereâ€™s the joke|2019-09| wheres|\n",
      "|f00o5zr|    Whereâ€™s the joke|2019-09|   joke|\n",
      "+-------+--------------------+-------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# **TODO** STEP 5: Recall that each row of the column cleaned_text contains a list of\n",
    "# tokens. We can't do anything with this. *Explode* this list so that each word appears\n",
    "# as a new row. Then *drop* the cleaned_text column since we don't need it anymore.\n",
    "result5 = result4.withColumn('word', explode('cleaned_text')).drop('cleaned_text')\n",
    "result5.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f25e790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+--------------------+------+\n",
      "|     id|   word|  month|                body|tf_num|\n",
      "+-------+-------+-------+--------------------+------+\n",
      "|f026ytg| couple|2019-09|thatâ€™s the minor ...|     1|\n",
      "|ez6zh9g|signing|2019-09|If Iâ€™m not mistak...|     1|\n",
      "|ez6zh9g| spring|2019-09|If Iâ€™m not mistak...|     1|\n",
      "|etih7xu|    mac|2019-07|Wow... my dumb as...|     1|\n",
      "|err90z5|      5|2019-06|4 or 5 for AB is ...|     2|\n",
      "+-------+-------+-------+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# **TODO** STEP 7: Count the number of  times a token appears in each comment.\n",
    "# How would you do it in SQL? If you're stuck, write out the SQL query and find the proper\n",
    "# algebraic operators in Spark. *Alias* the resulting column \"tf_num\"\n",
    "sel = ['id','word','month','body']\n",
    "\n",
    "token_count_in_comments = result5.groupBy(sel).count().select(*sel,F.col('count').alias('tf_num'))\n",
    "token_count_in_comments.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2360aeaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------------------+------+\n",
      "|     id|  month|                body|tf_den|\n",
      "+-------+-------+--------------------+------+\n",
      "|eig34t6|2019-03|I get where youâ€™r...|    27|\n",
      "|ezk2maf|2019-09|Even better advic...|    13|\n",
      "|egzmds8|2019-02|&gt; The time and...|    56|\n",
      "|eeav86c|2019-01|That is one way t...|    23|\n",
      "|f7dm3hy|2019-11|I'm infuriated th...|    27|\n",
      "+-------+-------+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# **TODO** STEP 8: Count the unique number of tokens in each comment. *Alias* the resulting\n",
    "# column \"tf_den\"\n",
    "sel = ['id','month','body']\n",
    "tokens_in_comment = token_count_in_comments.groupBy(sel).count().select(*sel, F.col('count').alias('tf_den'))\n",
    "tokens_in_comment.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed42860f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|  month|idf_num|\n",
      "+-------+-------+\n",
      "|2019-10|   5108|\n",
      "|2019-11|   4528|\n",
      "|2019-03|   6116|\n",
      "|2019-07|   4580|\n",
      "|2019-05|   5006|\n",
      "+-------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# **TODO** STEP 9: Count the number of comments in each month-year and alias the resulting\n",
    "# column as \"idf_num\".\n",
    "comments_in_month = tokens_in_comment.groupBy('month').count().select('month',F.col('count').alias('idf_num'))\n",
    "comments_in_month.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eaef81c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-------+\n",
      "|  month|    word|idf_den|\n",
      "+-------+--------+-------+\n",
      "|2019-03|   moved|      8|\n",
      "|2019-08|   youll|     89|\n",
      "|2019-02|textbook|     10|\n",
      "|2019-03|    type|     36|\n",
      "|2019-02|   check|     70|\n",
      "+-------+--------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# **TODO** STEP 10: For each month-year, calculate the number of comments each unique token appears in.\n",
    "# This is the denominator of the IDF term, so alias the resulting column \"idf_den\". IDF can be very\n",
    "# noisy, so filter so only tokens that have an idf_den > 7 are included. This also speeds up execution.\n",
    "sel = ['month','word']\n",
    "comments_with_token = token_count_in_comments.groupBy(sel).count().where(\"count > 7\").select(*sel, F.col('count').alias('idf_den'))\n",
    "comments_with_token.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "172d4745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-------+--------------------+------+------+-------+-------+\n",
      "|     id|    word|  month|                body|tf_num|tf_den|idf_num|idf_den|\n",
      "+-------+--------+-------+--------------------+------+------+-------+-------+\n",
      "|f026ytg|  couple|2019-09|thatâ€™s the minor ...|     1|     8|   6103|     65|\n",
      "|ez6zh9g|  spring|2019-09|If Iâ€™m not mistak...|     1|    17|   6103|     49|\n",
      "|err90z5|       5|2019-06|4 or 5 for AB is ...|     2|     9|   4781|     58|\n",
      "|f9teu3v|mistakes|2019-12|I guarantee almos...|     1|    35|   3718|      9|\n",
      "|ex0d0uv|personal|2019-08|I can't exactly s...|     1|    98|   4874|     34|\n",
      "+-------+--------+-------+--------------------+------+------+-------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# **TODO** STEP 11: You now have 4 different tables containing the components of TF-IDF. You need\n",
    "# to join them together. This data is small, but we still need to be very careful.\n",
    "# Before believing your join is correct, add .explain() at the end of your statement to make sure\n",
    "# Spark does the right thing. If it doesn't, you will may need to change a default (see Lecture 15)\n",
    "# and also provide a join hint to the optimizer. Hint: you probably don't need to worry about this,\n",
    "# if you do, use *broadcast*. The data is small enough that this shouldn't be an issue.\n",
    "# NOTE: This should take about 5-10 minutes on a modern system once you call .collect(), .head(),\n",
    "# or .show(). Remember, due to LAZY EVALUATION, the join command will return automatically.\n",
    "id_and_tf_den = tokens_in_comment.select('id','tf_den')\n",
    "word_month_idf_den = comments_with_token.select('word','month','idf_den')\n",
    "\n",
    "sel = ['L.id', 'word', 'month', 'body', 'tf_num', 'tf_den']\n",
    "num_den = token_count_in_comments.alias('L').join(broadcast(id_and_tf_den.alias('R')), col('L.id') == col('R.id')).select(sel)\n",
    "\n",
    "sel = ['L.id', 'word', 'L.month', 'L.body', 'tf_num', 'tf_den','idf_num']\n",
    "num_den_idf = num_den.alias('L').join(broadcast(comments_in_month.alias('R')), col('L.month') == col('R.month')).select(sel)\n",
    "\n",
    "\n",
    "sel = ['L.id', 'L.word', 'L.month', 'L.body', 'L.tf_num', 'L.tf_den','L.idf_num', 'idf_den']\n",
    "joined = num_den_idf.alias('L').join(broadcast(word_month_idf_den.alias('R')), (col('L.month') == col('R.month')) & (col('L.word') == col('R.word'))).select(sel)\n",
    "joined.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42243945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-------+--------------------+------+------+-------+-------+--------------------+\n",
      "|     id|    word|  month|                body|tf_num|tf_den|idf_num|idf_den|               tfidf|\n",
      "+-------+--------+-------+--------------------+------+------+-------+-------+--------------------+\n",
      "|f026ytg|  couple|2019-09|thatâ€™s the minor ...|     1|     8|   6103|     65|  0.5658601238147587|\n",
      "|ez6zh9g|  spring|2019-09|If Iâ€™m not mistak...|     1|    17|   6103|     49|  0.2826183957127264|\n",
      "|err90z5|       5|2019-06|4 or 5 for AB is ...|     2|     9|   4781|     58|  0.9766372366045297|\n",
      "|f9teu3v|mistakes|2019-12|I guarantee almos...|     1|    35|   3718|      9| 0.16909588786535268|\n",
      "|ex0d0uv|personal|2019-08|I can't exactly s...|     1|    98|   4874|     34|0.050370634415262634|\n",
      "+-------+--------+-------+--------------------+------+------+-------+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# **TODO** STEP 12: Compute TF-IDF. See the formula in the spec. The log function is imported for you.\n",
    "# There should be one TF-IDF value for each month-year/comment/token triple. Call the new column \"tfidf\".\n",
    "tfidf_df = joined.withColumn('tfidf', (joined.tf_num/joined.tf_den)*log(joined.idf_num/(1+joined.idf_den)))\n",
    "tfidf_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8106d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+-------------------+\n",
      "|    word|  month|          avg_tfidf|\n",
      "+--------+-------+-------------------+\n",
      "|accepted|2019-08|0.29177452105790985|\n",
      "|    push|2019-10| 0.7640444673835423|\n",
      "|yourself|2019-11|0.20341916475253005|\n",
      "| regents|2019-03| 0.4637794895577473|\n",
      "|    lazy|2019-03|0.24533689903449543|\n",
      "+--------+-------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# **TODO** STEP 13: Having a TF-IDF score for each token in each comment in each time period is correct;\n",
    "# however, it doesn't get us what we want. Let's now compute the average TF-IDF value of each token per month-year.\n",
    "# *Alias* the column \"avg_tfidf\".\n",
    "sel = ['word','month',F.col(\"avg(tfidf)\").alias('avg_tfidf')]\n",
    "avg_df = tfidf_df.groupBy('word', 'month').agg(F.avg('tfidf')).select(sel)\n",
    "avg_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d447861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------------------+\n",
      "|  month|     word|     max_avg_tfidf|\n",
      "+-------+---------+------------------+\n",
      "|2019-10|       ðŸ˜‚| 3.285918177327779|\n",
      "|2019-11|       ðŸ¦ƒ| 5.584822275826805|\n",
      "|2019-03|     amen|  5.53067225746926|\n",
      "|2019-07|copypasta|2.8365135111639743|\n",
      "|2019-05|      lt3|2.8182651663901304|\n",
      "|2019-08|     bruh| 2.487095325256426|\n",
      "|2019-01|      yep|1.9305518351414224|\n",
      "|2019-09|       ty| 3.732987144711901|\n",
      "|2019-06| messaged|3.5813949885185186|\n",
      "|2019-04|      wtf|3.2384386326871994|\n",
      "|2019-12|   please|3.0570872091281602|\n",
      "|2019-02|      wtf|2.6818136326206155|\n",
      "+-------+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# **TODO** STEP 13: Find the token(s) that have maximal average-TF-IDF within each month-year.\n",
    "# This is a mathematical argmax, and can be implemented as a join.\n",
    "# You will need to use a join on table avg_tfidf. Again, see the cautions in Step 11.\n",
    "max_tfidf = avg_df.groupBy('month').max('avg_tfidf').select('month',F.col('max(avg_tfidf)').alias('max_avg_tfidf'))\n",
    "# max_tfidf.show(5)\n",
    "max_joined = max_tfidf.alias('L').join(broadcast(avg_df).alias('R'),col('L.max_avg_tfidf') == col('R.avg_tfidf')).select('L.month','R.word','max_avg_tfidf')\n",
    "max_joined.show(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52d0ba7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|  month|     word|\n",
      "+-------+---------+\n",
      "|2019-01|      yep|\n",
      "|2019-02|      wtf|\n",
      "|2019-03|     amen|\n",
      "|2019-04|      wtf|\n",
      "|2019-05|      lt3|\n",
      "|2019-06| messaged|\n",
      "|2019-07|copypasta|\n",
      "|2019-08|     bruh|\n",
      "|2019-09|       ty|\n",
      "|2019-10|       ðŸ˜‚|\n",
      "|2019-11|       ðŸ¦ƒ|\n",
      "|2019-12|   please|\n",
      "+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Note that Spark may give you some problems in Step 13. If you get errors that you are referring to a non-existent\n",
    "# column, particularly in the join to create max_joined, what's happening is that Spark sees multiple copies\n",
    "# of the same column and isn't sure which column you're referring to. Use the following snippet (or something\n",
    "# similar to it) if that happens, before moving on to the final join.\n",
    "# max_tfidf = max_tfidf.select(F.col(\"month\").alias(\"month\"), \"max\")\n",
    "# Depending on how you wrote your code, you may not run into this issue.\n",
    "\n",
    "# **TODO** STEP 14: Finally, order the results by month and output just the month-year and token.\n",
    "# Note that some months may be missing due to the earlier filter on IDF.\n",
    "final_result = max_joined.orderBy(col('month').asc()).select('month','word')\n",
    "final_result.show()\n",
    "\n",
    "### EXECUTING MY SOLUTION TOOK APPROXIMATELY 10-15 MINUTES FROM TOP TO BOTTOM.\n",
    "# You should make sure a step works correctly before moving to the next.\n",
    "# This will save you time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f45b8db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
